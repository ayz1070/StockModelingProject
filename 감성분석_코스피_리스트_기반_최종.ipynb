{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "748d38f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahn\\anaconda3\\envs\\CapstonStockProject\\lib\\site-packages\\openpyxl\\styles\\stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['삼성전자', 'LG에너지솔루션', 'SK하이닉스', '삼성바이오로직스', 'POSCO홀딩스']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "kospi = pd.read_excel(\"kospi100.xlsx\", engine='openpyxl')\n",
    "\n",
    "searches = kospi['종목명'][:5].tolist()\n",
    "searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3934be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_avg_list = []\n",
    "keywords_list = []\n",
    "senti_count_list = []\n",
    "senti_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "629838ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at hyunwoongko/kobart and are newly initialized: ['classification_head.out_proj.bias', 'classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 크롤링시 필요한 라이브러리 불러오기\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from konlpy.tag import Hannanum  # Hannanum 형태소 분석기 불러오기\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# 페이지 입력 (1 페이지당 기사 10개 이하)\n",
    "def makePgNum(num):\n",
    "    if num == 1:\n",
    "        return num\n",
    "    elif num == 0:\n",
    "        return num + 1\n",
    "    else:\n",
    "        return num + 9 * (num - 1)\n",
    "\n",
    "\n",
    "# search : 검색어, pd=4 : 최근 1일, start_page : 몇 페이지\n",
    "def makeUrl(search, start_pg, end_pg):\n",
    "    if start_pg == end_pg:\n",
    "        start_page = makePgNum(start_pg)\n",
    "        # 정확도순(디폴트)으로 1일간의 뉴스(pd=4) \n",
    "        url = \"https://search.naver.com/search.naver?where=news&sm=tab_pge&query=\" + search +\"&start=\" + str(\n",
    "            start_page)\n",
    "        \n",
    "        return url\n",
    "    else:\n",
    "        # url 부분에서 정확도순서로 1일 데이터를 분류 가능\n",
    "        urls = []\n",
    "        for i in range(start_pg, end_pg + 1):\n",
    "            page = makePgNum(i)\n",
    "            \n",
    "            url = \"https://search.naver.com/search.naver?where=news&sm=tab_pge&query=\" + search +\"&pd=4\"+\"&start=\" + str(page)\n",
    "            #url = \"https://search.naver.com/search.naver?where=news&query=%EC%B9%B4%EC%B9%B4%EC%98%A4&sm=tab_opt&sort=0&photo=0&field=0&pd=3&ds=2021.09.10&de=2021.09.15&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Ar%2Cp%3Afrom20210910to20210915&is_sug_officeid=0\"+\"&start=\" + str(page)\n",
    "            urls.append(url)\n",
    "        return urls\n",
    "\n",
    "    # html에서 원하는 속성 추출하는 함수 만들기 (기사, 추출하려는 속성값)\n",
    "\n",
    "# 기사 내용 크롤링 함수\n",
    "def news_attrs_crawler(articles, attrs):\n",
    "    attrs_content = []\n",
    "    for i in articles:\n",
    "        attrs_content.append(i.attrs[attrs])\n",
    "    return attrs_content\n",
    "\n",
    "\n",
    "# ConnectionError방지\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/98.0.4758.102\"}\n",
    "\n",
    "# html생성해서 기사크롤링하는 함수 만들기(url): 링크를 반환\n",
    "def articles_crawler(url):\n",
    "    # html 불러오기\n",
    "    original_html = requests.get(i, headers=headers)\n",
    "    html = BeautifulSoup(original_html.text, \"html.parser\")\n",
    "\n",
    "    url_naver = html.select(\n",
    "        \"div.group_news > ul.list_news > li div.news_area > div.news_info > div.info_group > a.info\")\n",
    "    url = news_attrs_crawler(url_naver, 'href')\n",
    "    return url\n",
    "\n",
    "# 제목, 링크, 내용 1차원 리스트로 꺼내는 함수 생성\n",
    "def makeList(newlist, content):\n",
    "    for i in content:\n",
    "        for j in i:\n",
    "            newlist.append(j)\n",
    "    return newlist\n",
    "\n",
    "def preprocess(text):\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    inputs.to(device)\n",
    "    return inputs\n",
    "\n",
    "# 예측 함수\n",
    "def predict(inputs):\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probs = logits.softmax(dim=-1)\n",
    "    return probs[0].detach().cpu().numpy()\n",
    "\n",
    "# title을 예측해서 수치화시켜 sentiment으로 저장. 즉, title를 수치화 시킨 것이 sentiment\n",
    "\n",
    "# 0.5 기준으로 하면 부정확하긴 함... 정확도를 높이려면 이 부분 건들면 좋을 듯 또는 중립을 포함시키는 것도 해봐야 할 듯\n",
    "def convert_sentiment(probs):\n",
    "    if probs[0] < 0.5:\n",
    "        return 0\n",
    "    elif probs[0]>= 0.5:\n",
    "        return 1\n",
    "#     else:\n",
    "#         return '중립'\n",
    "# KoELECTRA 모델 로드\n",
    "model_name = \"hyunwoongko/kobart\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "hannanum = Hannanum()\n",
    "\n",
    "  # Hannanum 형태소 분석기 객체 생성\n",
    "def tokenize(text):\n",
    "    return hannanum.morphs(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0340fd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at hyunwoongko/kobart and are newly initialized: ['classification_head.out_proj.bias', 'classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 119524.22it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:04<00:00,  6.72it/s]\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at hyunwoongko/kobart and are newly initialized: ['classification_head.out_proj.bias', 'classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 131/131 [00:00<?, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 41/41 [00:06<00:00,  6.76it/s]\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at hyunwoongko/kobart and are newly initialized: ['classification_head.out_proj.bias', 'classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:00<?, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 56/56 [00:08<00:00,  6.38it/s]\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at hyunwoongko/kobart and are newly initialized: ['classification_head.out_proj.bias', 'classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:00<?, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 17/17 [00:02<00:00,  6.25it/s]\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at hyunwoongko/kobart and are newly initialized: ['classification_head.out_proj.bias', 'classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 69/69 [00:00<?, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [00:02<00:00,  6.99it/s]\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "#####뉴스크롤링 시작#####\n",
    "\n",
    "# 검색어 입력\n",
    "# search = input(\"검색 키워드 입력 : \")\n",
    "# 검색 시작할 페이지 입력\n",
    "page = 1\n",
    "# 검색 종료할 페이지 입력\n",
    "page2 = 10\n",
    "\n",
    "for search in searches:\n",
    "    # KoELECTRA 모델 로드\n",
    "    model_name = \"hyunwoongko/kobart\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # 형태소 분석 함수\n",
    "    hannanum = Hannanum()\n",
    "    # naver url 생성\n",
    "    url = makeUrl(search, page, page2)\n",
    "\n",
    "    # 뉴스 크롤러 실행\n",
    "    news_titles = []\n",
    "    news_url = []\n",
    "    news_contents = []\n",
    "    news_dates = []\n",
    "\n",
    "    for i in url:\n",
    "        url = articles_crawler(url)\n",
    "        news_url.append(url)\n",
    "\n",
    "\n",
    "\n",
    "    # 제목, 링크, 내용 담을 리스트 생성\n",
    "    news_url_1 = []\n",
    "\n",
    "    # 1차원 리스트로 만들기(내용 제외)\n",
    "    makeList(news_url_1, news_url)\n",
    "\n",
    "    # NAVER 뉴스만 남기기\n",
    "    final_urls = []\n",
    "    for i in tqdm(range(len(news_url_1))):\n",
    "        if \"news.naver.com\" in news_url_1[i]:\n",
    "            final_urls.append(news_url_1[i])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # 뉴스 내용 크롤링\n",
    "    for i in tqdm(final_urls):\n",
    "        # 각 기사 html get하기\n",
    "        news = requests.get(i, headers=headers)\n",
    "        news_html = BeautifulSoup(news.text, \"html.parser\")\n",
    "\n",
    "        # 뉴스 제목 가져오기\n",
    "        title = news_html.select_one(\"#ct > div.media_end_head.go_trans > div.media_end_head_title > h2\")\n",
    "        if title == None:\n",
    "            title = news_html.select_one(\"#content > div.end_ct > div > h2\")\n",
    "    # ------------------------------------------------------------------------------------------------------------------\n",
    "        # 뉴스 본문 가져오기 (일단 구현은 해놓음 but 일단 기사 제목 수준에서 진행)\n",
    "        content = news_html.select(\"div#dic_area\")\n",
    "        if content == []:\n",
    "            content = news_html.select(\"#articeBody\")\n",
    "        content = ''.join(str(content))\n",
    "\n",
    "        # html태그제거 및 텍스트 다듬기\n",
    "        pattern1 = '<[^>]*>'\n",
    "        title = re.sub(pattern=pattern1, repl='', string=str(title))\n",
    "        content = re.sub(pattern=pattern1, repl='', string=content)\n",
    "        pattern2 = \"\"\"[\\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\"\"\"\n",
    "        content = content.replace(pattern2, '')\n",
    "\n",
    "        news_titles.append(title)\n",
    "        news_contents.append(content)\n",
    "\n",
    "        try:\n",
    "            html_date = news_html.select_one(\n",
    "                \"div#ct> div.media_end_head.go_trans > div.media_end_head_info.nv_notrans > div.media_end_head_info_datestamp > div > span\")\n",
    "            news_date = html_date.attrs['data-date-time']\n",
    "        except AttributeError:\n",
    "            news_date = news_html.select_one(\"#content > div.end_ct > div > div.article_info > span > em\")\n",
    "            news_date = re.sub(pattern=pattern1, repl='', string=str(news_date))\n",
    "            news_date = news_date[0:10]\n",
    "        # 날짜 가져오기\n",
    "        news_dates.append(news_date)\n",
    "    df = pd.DataFrame({'date': news_dates, 'title': news_titles, 'content' : news_contents}) #'url' : final_urls\n",
    "    \n",
    "    # title 열에 대해 형태소 분석 적용\n",
    "\n",
    "    df['title'] = df['title'].astype(str).apply(tokenize)\n",
    "    # for i in range(df.shape[0]):\n",
    "    #     df.iloc[i,1] = df.iloc[i,1].apply(tokenize)\n",
    "\n",
    "\n",
    "    # title을 예측해서 수치화시켜 sentiment으로 저장. 즉, title를 수치화 시킨 것이 sentiment\n",
    "    df['sentiment'] = df['title'].apply(lambda x: predict(preprocess(' '.join(x))))\n",
    "\n",
    "\n",
    "    # train할 label은 제목을 읽고 내가 직접 라벨링\n",
    "    # test할 label은 0.5를 기준으로 sentiment가 0.5보다 크면 1, 작으면 0으로 기준 세움\n",
    "    df['label'] = df['sentiment'].apply(convert_sentiment)\n",
    "    df['sentiment'] = df['sentiment'].apply(lambda x: x[0]).tolist()\n",
    "    df=round(df,2)\n",
    "    # 데이터 프레임 저장\n",
    "    now = datetime.datetime.now()\n",
    "    date = str(now.year%100)+format(now.month,'02')+format(now.day,'02')\n",
    "    #df.to_csv(f'csv/{date}_{search}_감성분석.csv', encoding='utf-8-sig', index=False)\n",
    "#     평균으로 접근하면 너무 편차가 크다\n",
    "    senti_avg = df['sentiment'].mean()\n",
    "    senti_avg_list.append(senti_avg)\n",
    "    \n",
    "    \n",
    "    if df[df['label'] == 1].label.count() > df[df['label'] == 0].label.count():\n",
    "        senti_count = 1\n",
    "    else:\n",
    "        senti_count = 0\n",
    "    senti_count_list.append(senti_count)\n",
    "\n",
    "\n",
    "    \n",
    "    sentimented_title = []\n",
    "    for i in range(len(df['title'])):\n",
    "        for j in range(len(df['title'][i])):\n",
    "            if len(df['title'][i][j]) != 1: \n",
    "                sentimented_title.append(df['title'][i][j])\n",
    "\n",
    "    counter = Counter(sentimented_title)\n",
    "\n",
    "    keywords = []\n",
    "    most_common_items = counter.most_common(5)  # 리스트의 길이를 먼저 확인\n",
    "    for i in range(min(5, len(most_common_items))):\n",
    "        keywords.append(most_common_items[i][0])\n",
    "    keywords_list.append(keywords)\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "952c818e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['삼성전자', '코스피', 'IFA', '용량', '2023'], ['코스피', 'LG', 'IFA', '시황', '마감'], ['코스피', '삼성전자', '마감', '하락', '상승'], ['코스피', '마감', '시황', '하락', '삼성전자'], ['코스피', '마감', '시황', '삼성전자', '2550대']]\n",
      "[0.3549999999999999, 0.42200000000000004, 0.5970731707317073, 0.36892857142857144, 0.5994117647058823, 0.5647368421052631]\n",
      "[0, 1, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(keywords_list)\n",
    "print(senti_avg_list)\n",
    "print(senti_count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa89e4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'삼성전자': 0.3549999999999999,\n",
       " 'LG에너지솔루션': 0.42200000000000004,\n",
       " 'SK하이닉스': 0.5970731707317073,\n",
       " '삼성바이오로직스': 0.36892857142857144,\n",
       " 'POSCO홀딩스': 0.5994117647058823}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = searches\n",
    "keywords_dict = dict(zip(keys, keywords_list))\n",
    "senti_avg_dict = dict(zip(keys, senti_avg_list))\n",
    "senti_count_dict = dict(zip(keys, senti_count_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0181a781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "성공\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "data = {\n",
    "    'keywords': keywords_dict,\n",
    "    'senti_avg': senti_avg_dict,\n",
    "    'senti_count' : senti_count_dict \n",
    "}\n",
    "#https://616d-39-118-146-59.ngrok-free.app/sentiment\n",
    "server_url = 'http://127.0.0.1:5000/sentiment'\n",
    "\n",
    "response = requests.post(server_url, json=data)  # POST 요청으로 변경, 헤더는 자동으로 설정됨\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print('성공')\n",
    "else:\n",
    "    print('실패:', response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfcbd02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91400893",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CapstonStockProject",
   "language": "python",
   "name": "capstonestockproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
