{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24ec56d5",
   "metadata": {},
   "source": [
    "# 요약\n",
    "\n",
    "- 개요\n",
    "    - 주식의 가격에는 다양한 요소들이 영향을 미치지만 그 중 뉴스에 민감하다고 판단하여 뉴스 기사의 제목을 분석하여 긍정/부정 평가를 한다.\n",
    "1. 데이터 수집\n",
    "    - 최근 1일, 정확도 순서로 검색어 입력 시 그에 대한 뉴스 제목 정보를 크롤링한다.\n",
    "2. 전처리\n",
    "    - konlpy 라이브러리로 제목에 대해서 형태소 분석을 하고 tokenize를 진행한다.\n",
    "    - 제목을 수치화한 'sentiment' 값을 얻어내고 데이터는 아래와 같다.\n",
    "    - feature\n",
    "        - 크롤링한 기사 제목에 대한 bert 모델을 적용한 수치값 (sentiment)\n",
    "    - target\n",
    "        - 긍정인 경우 1, 부정인 경우 0으로 라벨링 설정\n",
    "3. 모델링\n",
    "    - 삼성전자, 카카오, 네이버, sk하이닉스, 현대자동차의 기사 제목의 sentiment 값을 구하고 600개 정도의 데이터를 직접 라벨링 함. (계속 추가할 예정)\n",
    "    - DecisionTree, RandomForestClassifier, LogisticRegression 모델을 학습시켜 0.84, 0.85, 0.86의  accuracy가 나옴.\n",
    "    \n",
    "4. 성능 평가\n",
    "    - 최근 삼성의 1일 뉴스 데이터를 적용해본 결과 실제로 긍정적인 뉴스가 많았고 긍정을 예측함.\n",
    "    - 우리의 목적은 뉴스에 대한 긍정 평가였지만 실제로 주가의 상승으로 이어짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8bf180",
   "metadata": {},
   "source": [
    "# 네이버 뉴스 크롤링\n",
    "- 검색어 입력 받고 정확도순, 최근 1일 데이터 수집\n",
    "- 제목 수준에서 긍/부정 판단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d0a912b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 횟수 입력 : 777\n",
      "검색 키워드 입력 : 삼성전자\n",
      "\n",
      "크롤링할 시작 페이지를 입력해주세요. ex)1(숫자만입력):1\n",
      "\n",
      "크롤링할 시작 페이지:  1 페이지\n",
      "\n",
      "크롤링할 종료 페이지를 입력해주세요. ex)1(숫자만입력):10\n",
      "\n",
      "크롤링할 종료 페이지:  10 페이지\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 147/147 [00:00<?, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 47/47 [00:12<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색된 기사 갯수: 총  100 개\n",
      "\n",
      "[뉴스 제목]\n",
      "['삼성전자, 올림픽의 날 앞두고 운동 캠페인', \"삼성전자, 'AI로 에너지 절약하는' 비스포크 신제품 공개\", \"삼성전자, 전 세계에 '비스포크 라이프' 가전 늘린다\", '[특징주] 삼성전자, 3거래일 연속 하락…아슬한 ‘7만전자’', '[단독] 한진만 삼성전자 미주총괄 부사장 \"美 칩 인력 부족 심각…해결책 절실\"', '삼성전자, 3거래일 연속 하락세…‘7만전자’ 턱걸이[특징주]', \"반도체 패권 잡자… 삼성전자·TSMC '기술경쟁' 격화\", \"현대차에 삼성전자 '최첨단 두뇌' 심는다 [삼성-현대차, 모빌리티 반도체 동맹]\", '삼성전자 시스템 반도체가 들어간 현대차를 타본다면', '이재용·정의선 손잡았다…삼성전자 車반도체, 현대차에 탑재', '경계현 삼성전자 사장과 인사하는 김기현 대표', '참석자와 대화하는 경계현 삼성전자 DS부문장', \"'삼성전자와 이탈리아 토일렛페이퍼의 만남'\", '[스페셜리포트]삼성전자·LGD ‘OLED 동맹’...韓 대형 디스플레이 주도권 되찾는다', \"삼성전자, '글로벌 비스포크 라이프 2023' 온라인 행사 개최\", \"삼성전자만 10조 이상 폭풍매수…외국인 쏠림현상에 증시 '착시'\", '삼성전자, 현대차에 차량용칩 공급…인포테인먼트 분야 첫 협력', '美中 반도체戰에 낀 삼성전자…‘7만전자’, 엔비디아 ‘조정’ 여파로 흔들리나 [투자360]', \"환경산업기술원, 삼성전자 등과 탄소 데이터베이스 구축 '맞손'\", '삼성전자 “서울서 갤럭시 언팩, 전세계에 폴더블폰 원조 각인”', '디아이, 삼성전자와 35억 규모 반도체 검사보드 계약', \"'삼성 AP 특화 파트너'... 코아시아, 삼성전자의 현대차 AP 최초 공급 수혜 기대\", '디아이, 삼성전자와 35억원 규모 반도체 검사보드 공급 계약', \"삼성전자, 프린터 복합기 구매 시 '정품토너' 증정 실시\", '[단독]韓, 기술 탈취범 80%가 집유… 美선 ‘경제스파이’ 간주 30년형도', '현대차·삼성·SK, 불황에도 작년 고용 늘렸다', '윤 대통령 \"반도체 경쟁은 국가총력전\"…\\'전쟁\\'만 4번 언급(종합)', \"[단독] 현대차 턱밑 추격하는 쿠팡…올해 고용 증가 '1위'\", '삼성, 2025년 현대차에 인포테인먼트 칩 공급', '10만 걸음 도전 목표…삼성 헬스, 올림픽의 날 기념 `스텝 챌린지` 개최', '한종희 \"비스포크 라이프로 지속 가능한 집과 미래 제시\"', \"[단독] 삼성 33조 투자에 환호…소 키우던 시골 도시 '천지개벽'\", \"삼성 또 넘었다…SK하이닉스, '세계 최고층' 238단 낸드 생산\", '삼전 개미들 본전 찾을 때…네카오 90%가 물려있다', \"이재용-정의선, 3년전 의기투합… '미래차 경쟁력 강화' 결실 [삼성-현대차, 모빌리티 반도체 동맹]\", \"지난해 고용창출 1위 현대차 '1.4만명'…쿠팡은 2만명 사라졌다\", '스마트 워치로 일상서 혈압-심전도 측정… 자는 동안 수면분석도', \"10만 걸음 걸어볼까…삼성 헬스, 올림픽의 날 기념 '스텝 챌린지' 개최\", '아우디·폭스바겐서 검증한 삼성의 엑시노스 현대차에도 탑재(종합)', '갤럭시Z플립 5, 지금까지 알려진 모든 것', \"[삼성 신경영 30주년 (상)] 이건희의 '질(質) 삼성'…초격차의 시작\", '이재용·정의선 손잡았다…차량용 인포테인먼트 칩 공급', '82개 그룹 직원 1년새 4만3천명 증가…현대차그룹 1만4천명↑', '디자인 입은 비스포크 냉장고', \"이재용·정의선 '맞손'…차량용 인포테인먼트 첫 협력(종합)\", '대기업 지난해 고용 4.3만명↑… 현대차 1만명 늘고 쿠팡 2만명 줄고', \"삼성의 첫 '안방 언팩' 이유는...'폴더블 선구자'의 존재감, 세계에 과시\"]\n",
      "\n",
      "[뉴스 링크]\n",
      "['https://n.news.naver.com/mnews/article/001/0013988695?sid=101', 'https://n.news.naver.com/mnews/article/293/0000044775?sid=105', 'https://n.news.naver.com/mnews/article/003/0011901808?sid=101', 'https://n.news.naver.com/mnews/article/366/0000907566?sid=101', 'https://n.news.naver.com/mnews/article/011/0004199403?sid=101', 'https://n.news.naver.com/mnews/article/018/0005503301?sid=101', 'https://n.news.naver.com/mnews/article/417/0000926303?sid=101', 'https://n.news.naver.com/mnews/article/014/0005023883?sid=101', 'https://n.news.naver.com/mnews/article/469/0000743313?sid=101', 'https://n.news.naver.com/mnews/article/009/0005140731?sid=101', 'https://n.news.naver.com/mnews/article/001/0013989673?sid=101', 'https://n.news.naver.com/mnews/article/001/0013989674?sid=101', 'https://n.news.naver.com/mnews/article/421/0006853138?sid=101', 'https://n.news.naver.com/mnews/article/030/0003105618?sid=105', 'https://n.news.naver.com/mnews/article/001/0013988623?sid=101', 'https://n.news.naver.com/mnews/article/277/0005269594?sid=101', 'https://n.news.naver.com/mnews/article/008/0004896386?sid=101', 'https://n.news.naver.com/mnews/article/016/0002153422?sid=101', 'https://n.news.naver.com/mnews/article/011/0004199006?sid=101', 'https://n.news.naver.com/mnews/article/020/0003502210?sid=101', 'https://n.news.naver.com/mnews/article/003/0011902395?sid=101', 'https://n.news.naver.com/mnews/article/014/0005024175?sid=101', 'https://n.news.naver.com/mnews/article/417/0000926446?sid=101', 'https://n.news.naver.com/mnews/article/092/0002294735?sid=105', 'https://n.news.naver.com/mnews/article/020/0003502140?sid=101', 'https://n.news.naver.com/mnews/article/277/0005269840?sid=101', 'https://n.news.naver.com/mnews/article/421/0006854026?sid=100', 'https://n.news.naver.com/mnews/article/015/0004853645?sid=101', 'https://n.news.naver.com/mnews/article/030/0003105594?sid=105', 'https://n.news.naver.com/mnews/article/029/0002805465?sid=105', 'https://n.news.naver.com/mnews/article/015/0004853639?sid=101', 'https://n.news.naver.com/mnews/article/015/0004853273?sid=104', 'https://n.news.naver.com/mnews/article/138/0002150007?sid=105', 'https://n.news.naver.com/mnews/article/008/0004896450?sid=101', 'https://n.news.naver.com/mnews/article/014/0005023825?sid=101', 'https://n.news.naver.com/mnews/article/421/0006853706?sid=101', 'https://n.news.naver.com/mnews/article/020/0003502160?sid=101', 'https://n.news.naver.com/mnews/article/138/0002150026?sid=105', 'https://n.news.naver.com/mnews/article/277/0005269361?sid=101', 'https://n.news.naver.com/mnews/article/092/0002294661?sid=105', 'https://n.news.naver.com/mnews/article/088/0000818402?sid=101', 'https://n.news.naver.com/mnews/article/243/0000045917?sid=103', 'https://n.news.naver.com/mnews/article/001/0013989319?sid=101', 'https://n.news.naver.com/mnews/article/421/0006853159?sid=101', 'https://n.news.naver.com/mnews/article/001/0013987989?sid=105', 'https://n.news.naver.com/mnews/article/417/0000926406?sid=101', 'https://n.news.naver.com/mnews/article/008/0004896592?sid=105']\n",
      "news_title:  47\n",
      "news_url:  47\n",
      "news_dates:  47\n",
      "중복 제거 후 행 개수:  47\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# 크롤링시 필요한 라이브러리 불러오기\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 페이지 입력 (1 페이지당 기사 10개 이하)\n",
    "def makePgNum(num):\n",
    "    if num == 1:\n",
    "        return num\n",
    "    elif num == 0:\n",
    "        return num + 1\n",
    "    else:\n",
    "        return num + 9 * (num - 1)\n",
    "\n",
    "\n",
    "# search : 검색어, pd=4 : 최근 1일, start_page : 몇 페이지\n",
    "def makeUrl(search, start_pg, end_pg):\n",
    "    if start_pg == end_pg:\n",
    "        start_page = makePgNum(start_pg)\n",
    "        # 정확도순(디폴트)으로 1일간의 뉴스(pd=4) \n",
    "        url = \"https://search.naver.com/search.naver?where=news&sm=tab_pge&query=\" + search +\"&pd=4\"+\"&start=\" + str(\n",
    "            start_page)\n",
    "        \n",
    "        return url\n",
    "    else:\n",
    "        # url 부분에서 정확도순서로 1일 데이터를 분류 가능\n",
    "        urls = []\n",
    "        for i in range(start_pg, end_pg + 1):\n",
    "            page = makePgNum(i)\n",
    "            \n",
    "            url = \"https://search.naver.com/search.naver?where=news&sm=tab_pge&query=\" + search +\"&pd=4\"+\"&start=\" + str(page)\n",
    "            #url = \"https://search.naver.com/search.naver?where=news&query=%EC%B9%B4%EC%B9%B4%EC%98%A4&sm=tab_opt&sort=0&photo=0&field=0&pd=3&ds=2021.09.10&de=2021.09.15&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Ar%2Cp%3Afrom20210910to20210915&is_sug_officeid=0\"+\"&start=\" + str(page)\n",
    "            urls.append(url)\n",
    "        return urls\n",
    "\n",
    "    # html에서 원하는 속성 추출하는 함수 만들기 (기사, 추출하려는 속성값)\n",
    "\n",
    "# 기사 내용 크롤링 함수\n",
    "def news_attrs_crawler(articles, attrs):\n",
    "    attrs_content = []\n",
    "    for i in articles:\n",
    "        attrs_content.append(i.attrs[attrs])\n",
    "    return attrs_content\n",
    "\n",
    "\n",
    "# ConnectionError방지\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/98.0.4758.102\"}\n",
    "\n",
    "# html생성해서 기사크롤링하는 함수 만들기(url): 링크를 반환\n",
    "def articles_crawler(url):\n",
    "    # html 불러오기\n",
    "    original_html = requests.get(i, headers=headers)\n",
    "    html = BeautifulSoup(original_html.text, \"html.parser\")\n",
    "\n",
    "    url_naver = html.select(\n",
    "        \"div.group_news > ul.list_news > li div.news_area > div.news_info > div.info_group > a.info\")\n",
    "    url = news_attrs_crawler(url_naver, 'href')\n",
    "    return url\n",
    "\n",
    "\n",
    "#####뉴스크롤링 시작#####\n",
    "\n",
    "# 테스트 횟수\n",
    "num = int(input('테스트 횟수 입력 : '))\n",
    "# 검색어 입력\n",
    "search = input(\"검색 키워드 입력 : \")\n",
    "# 검색 시작할 페이지 입력\n",
    "page = int(input(\"\\n크롤링할 시작 페이지를 입력해주세요. ex)1(숫자만입력):\"))  # ex)1 =1페이지,2=2페이지...\n",
    "print(\"\\n크롤링할 시작 페이지: \", page, \"페이지\")\n",
    "# 검색 종료할 페이지 입력\n",
    "page2 = int(input(\"\\n크롤링할 종료 페이지를 입력해주세요. ex)1(숫자만입력):\"))  # ex)1 =1페이지,2=2페이지...\n",
    "print(\"\\n크롤링할 종료 페이지: \", page2, \"페이지\")\n",
    "\n",
    "# naver url 생성\n",
    "url = makeUrl(search, page, page2)\n",
    "\n",
    "# 뉴스 크롤러 실행\n",
    "news_titles = []\n",
    "news_url = []\n",
    "\n",
    "# 일단 제목 수준으로 진행 (기사 내용은 추후에 적용 여부 판단)\n",
    "# news_contents = []\n",
    "\n",
    "news_dates = []\n",
    "for i in url:\n",
    "    url = articles_crawler(url)\n",
    "    news_url.append(url)\n",
    "\n",
    "\n",
    "# 제목, 링크, 내용 1차원 리스트로 꺼내는 함수 생성\n",
    "def makeList(newlist, content):\n",
    "    for i in content:\n",
    "        for j in i:\n",
    "            newlist.append(j)\n",
    "    return newlist\n",
    "\n",
    "\n",
    "# 제목, 링크, 내용 담을 리스트 생성\n",
    "news_url_1 = []\n",
    "\n",
    "# 1차원 리스트로 만들기(내용 제외)\n",
    "makeList(news_url_1, news_url)\n",
    "\n",
    "# NAVER 뉴스만 남기기\n",
    "final_urls = []\n",
    "for i in tqdm(range(len(news_url_1))):\n",
    "    if \"news.naver.com\" in news_url_1[i]:\n",
    "        final_urls.append(news_url_1[i])\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# 뉴스 내용 크롤링\n",
    "for i in tqdm(final_urls):\n",
    "    # 각 기사 html get하기\n",
    "    news = requests.get(i, headers=headers)\n",
    "    news_html = BeautifulSoup(news.text, \"html.parser\")\n",
    "\n",
    "    # 뉴스 제목 가져오기\n",
    "    title = news_html.select_one(\"#ct > div.media_end_head.go_trans > div.media_end_head_title > h2\")\n",
    "    if title == None:\n",
    "        title = news_html.select_one(\"#content > div.end_ct > div > h2\")\n",
    "# ------------------------------------------------------------------------------------------------------------------\n",
    "    # 뉴스 본문 가져오기 (일단 구현은 해놓음 but 일단 기사 제목 수준에서 진행)\n",
    "#     content = news_html.select(\"div#dic_area\")\n",
    "#     if content == []:\n",
    "#         content = news_html.select(\"#articeBody\")\n",
    "\n",
    "    # 기사 텍스트만 가져오기\n",
    "    # list합치기\n",
    "    #content = ''.join(str(content))\n",
    "\n",
    "    # html태그제거 및 텍스트 다듬기\n",
    "    pattern1 = '<[^>]*>'\n",
    "    title = re.sub(pattern=pattern1, repl='', string=str(title))\n",
    "#     content = re.sub(pattern=pattern1, repl='', string=content)\n",
    "#     pattern2 = \"\"\"[\\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\"\"\"\n",
    "#     content = content.replace(pattern2, '')\n",
    "    \n",
    "    news_titles.append(title)\n",
    "#    news_contents.append(content)\n",
    "\n",
    "    try:\n",
    "        html_date = news_html.select_one(\n",
    "            \"div#ct> div.media_end_head.go_trans > div.media_end_head_info.nv_notrans > div.media_end_head_info_datestamp > div > span\")\n",
    "        news_date = html_date.attrs['data-date-time']\n",
    "    except AttributeError:\n",
    "        news_date = news_html.select_one(\"#content > div.end_ct > div > div.article_info > span > em\")\n",
    "        news_date = re.sub(pattern=pattern1, repl='', string=str(news_date))\n",
    "    # 날짜 가져오기\n",
    "    news_dates.append(news_date)\n",
    "\n",
    "print(\"검색된 기사 갯수: 총 \", (page2 + 1 - page) * 10, '개')\n",
    "print(\"\\n[뉴스 제목]\")\n",
    "print(news_titles)\n",
    "print(\"\\n[뉴스 링크]\")\n",
    "print(final_urls)\n",
    "#print(\"\\n[뉴스 내용]\")\n",
    "#print(news_contents)\n",
    "\n",
    "print('news_title: ', len(news_titles))\n",
    "print('news_url: ', len(final_urls))\n",
    "#print('news_contents: ', len(news_contents))\n",
    "print('news_dates: ', len(news_dates))\n",
    "\n",
    "###데이터 프레임으로 만들기###\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터 프레임 만들기\n",
    "news_df = pd.DataFrame({'date': news_dates, 'title': news_titles})\n",
    "# news_df = pd.DataFrame({'date': news_dates, 'title': news_titles, 'link': final_urls, 'content': news_contents})\n",
    "\n",
    "# 중복 행 지우기\n",
    "news_df = news_df.drop_duplicates(keep='first', ignore_index=True)\n",
    "print(\"중복 제거 후 행 개수: \", len(news_df))\n",
    "\n",
    "# 데이터 프레임 저장\n",
    "now = datetime.datetime.now()\n",
    "news_df.to_csv(f'csv/{search} 뉴스 제목{num}.csv', encoding='utf-8-sig', index=False)\n",
    "\n",
    "print(type(news_df['title']))\n",
    "# print(type(news_df['content']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f76a8de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-09-15 20:12:22</td>\n",
       "      <td>[단독] 김범수 동생, 카카오 '옥상옥 지주사'서 퇴직금만 14억</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-09-15 06:01:03</td>\n",
       "      <td>'카카오 세상' 꿈꿨지만…택시 90% 완전 장악이 독 됐다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-09-15 19:52:41</td>\n",
       "      <td>[단독]국회 정무위, 카카오 김범수 '공정위 국감' 증인대 세운다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-09-15 09:29:19</td>\n",
       "      <td>카카오엔터테인먼트, 경력 개발자 첫 공개채용…최대 세 자릿수 규모</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-09-15 07:27:02</td>\n",
       "      <td>카카오 김범수 공정위 조사…“고의성 입증시 검찰 고발”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>2021-09-15 13:00:23</td>\n",
       "      <td>전방위 압박에 카카오 상생방안 발표...규제 피할까?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2021-09-15 11:53:18</td>\n",
       "      <td>착잡·당혹…뒤숭숭한 카카오 직원들</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2021-09-15 11:45:33</td>\n",
       "      <td>네이버·카카오페이, '소상공인 수수료' 카드사보다 최대 3배 높아</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2021-09-15 03:02:37</td>\n",
       "      <td>백기 든 카카오, 골목상권 손뗀다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2021-09-15 08:25:31</td>\n",
       "      <td>[그래픽] 공정위 조사받는 카카오 김범수 의장, 주요 쟁점</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   date                                 title\n",
       "0   2021-09-15 20:12:22  [단독] 김범수 동생, 카카오 '옥상옥 지주사'서 퇴직금만 14억\n",
       "1   2021-09-15 06:01:03      '카카오 세상' 꿈꿨지만…택시 90% 완전 장악이 독 됐다\n",
       "2   2021-09-15 19:52:41  [단독]국회 정무위, 카카오 김범수 '공정위 국감' 증인대 세운다\n",
       "3   2021-09-15 09:29:19  카카오엔터테인먼트, 경력 개발자 첫 공개채용…최대 세 자릿수 규모\n",
       "4   2021-09-15 07:27:02        카카오 김범수 공정위 조사…“고의성 입증시 검찰 고발”\n",
       "..                  ...                                   ...\n",
       "94  2021-09-15 13:00:23         전방위 압박에 카카오 상생방안 발표...규제 피할까?\n",
       "95  2021-09-15 11:53:18                    착잡·당혹…뒤숭숭한 카카오 직원들\n",
       "96  2021-09-15 11:45:33  네이버·카카오페이, '소상공인 수수료' 카드사보다 최대 3배 높아\n",
       "97  2021-09-15 03:02:37                    백기 든 카카오, 골목상권 손뗀다\n",
       "98  2021-09-15 08:25:31      [그래픽] 공정위 조사받는 카카오 김범수 의장, 주요 쟁점\n",
       "\n",
       "[99 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a22f29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CapstonStockProject",
   "language": "python",
   "name": "capstonestockproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
